{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7db8f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reaction-Diffusion Prediction with FNO3D\n",
    "# 1. Imports\n",
    "from neuralop.models import FNO  # FNO handles N-D input\n",
    "from neuralop.training import Trainer\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import h5py\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5b2c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Dataset class \n",
    "class ReactionDiffusionDataset3D(Dataset):\n",
    "    def __init__(self, file_path, initial_steps=10, future_steps=1):\n",
    "        self.file_path = file_path\n",
    "        self.initial_steps = initial_steps\n",
    "        self.future_steps = future_steps\n",
    "\n",
    "        # Open HDF5 in read-only mode\n",
    "        self.h5_file = h5py.File(file_path, \"r\")\n",
    "        self.keys = sorted(self.h5_file.keys())\n",
    "        self.samples_per_key = self.h5_file[self.keys[0]][\"data\"].shape[0] - initial_steps - future_steps + 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.keys) * self.samples_per_key\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        key_idx = idx // self.samples_per_key\n",
    "        local_idx = idx % self.samples_per_key\n",
    "        data = self.h5_file[self.keys[key_idx]][\"data\"][:]\n",
    "\n",
    "        x = data[local_idx:local_idx+self.initial_steps]\n",
    "        y = data[local_idx+self.initial_steps:local_idx+self.initial_steps+self.future_steps]\n",
    "\n",
    "        # Permute to [channels, nx, ny, time]\n",
    "        x = np.transpose(x, (3, 1, 2, 0))\n",
    "        y = np.transpose(y, (3, 1, 2, 0))\n",
    "\n",
    "        return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fcb43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Hyperparameters\n",
    "file_path = \"--ENTER FILE NAME HERE --.h5\"\n",
    "initial_steps = 10\n",
    "future_steps = 10\n",
    "batch_size = 4\n",
    "n_epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39887a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap dataset to return dict instead of tuple\n",
    "class DatasetWrapper(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x, y = self.dataset[idx]   \n",
    "        return {\n",
    "            \"x\": x.detach().clone().float(),\n",
    "            \"y\": y.detach().clone().float()\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1799abd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Dataset and DataLoader\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "dataset = ReactionDiffusionDataset3D(file_path, initial_steps=initial_steps, future_steps=future_steps)\n",
    "\n",
    "n_train = 500\n",
    "n_test = 50\n",
    "\n",
    "# Random permutation of indices\n",
    "perm = torch.randperm(len(dataset))\n",
    "\n",
    "train_indices = perm[:n_train]\n",
    "test_indices  = perm[n_train:n_train + n_test]\n",
    "\n",
    "train_dataset = Subset(dataset, train_indices)\n",
    "test_dataset  = Subset(dataset, test_indices)\n",
    "\n",
    "# Wrap train and test datasets\n",
    "train_dataset_wrapped = DatasetWrapper(train_dataset)\n",
    "test_dataset_wrapped = DatasetWrapper(test_dataset)\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(train_dataset_wrapped, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset_wrapped, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9be888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Initialize FNO3D model\n",
    "operator = FNO(\n",
    "    n_modes=(16,16,5),       # 3D Fourier modes: nx, ny, time\n",
    "    hidden_channels=32,\n",
    "    in_channels=dataset[0][0].shape[0],  # number of input channels\n",
    "    out_channels=dataset[0][1].shape[0]  # number of output channels\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e774c8ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 500 samples\n",
      "Testing on [50] samples         on resolutions ['test'].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\phroz\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786: UserWarning: FNO.forward() received unexpected keyword arguments: ['y']. These arguments will be ignored.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw outputs of shape torch.Size([4, 2, 128, 128, 10])\n",
      "[0] time=663.93, avg_loss=0.0059, train_err=0.0238\n",
      "Eval: test_mse=0.0019\n",
      "[1] time=657.45, avg_loss=0.0012, train_err=0.0049\n",
      "Eval: test_mse=0.0010\n",
      "[2] time=655.83, avg_loss=0.0006, train_err=0.0026\n",
      "Eval: test_mse=0.0006\n",
      "[3] time=657.76, avg_loss=0.0004, train_err=0.0017\n",
      "Eval: test_mse=0.0004\n",
      "[4] time=657.42, avg_loss=0.0003, train_err=0.0012\n",
      "Eval: test_mse=0.0003\n",
      "[5] time=668.47, avg_loss=0.0002, train_err=0.0010\n",
      "Eval: test_mse=0.0003\n",
      "[6] time=770.51, avg_loss=0.0002, train_err=0.0008\n",
      "Eval: test_mse=0.0002\n",
      "[7] time=723.86, avg_loss=0.0002, train_err=0.0007\n",
      "Eval: test_mse=0.0002\n",
      "[8] time=665.19, avg_loss=0.0002, train_err=0.0007\n",
      "Eval: test_mse=0.0002\n",
      "[9] time=630.12, avg_loss=0.0002, train_err=0.0006\n",
      "Eval: test_mse=0.0002\n",
      "[10] time=663.06, avg_loss=0.0001, train_err=0.0005\n",
      "Eval: test_mse=0.0002\n",
      "[11] time=640.90, avg_loss=0.0001, train_err=0.0005\n",
      "Eval: test_mse=0.0002\n",
      "[12] time=677.24, avg_loss=0.0001, train_err=0.0005\n",
      "Eval: test_mse=0.0001\n",
      "[13] time=684.78, avg_loss=0.0001, train_err=0.0005\n",
      "Eval: test_mse=0.0002\n",
      "[14] time=661.38, avg_loss=0.0001, train_err=0.0005\n",
      "Eval: test_mse=0.0001\n",
      "[15] time=659.25, avg_loss=0.0001, train_err=0.0005\n",
      "Eval: test_mse=0.0001\n",
      "[16] time=652.20, avg_loss=0.0001, train_err=0.0005\n",
      "Eval: test_mse=0.0001\n",
      "[17] time=751.45, avg_loss=0.0001, train_err=0.0004\n",
      "Eval: test_mse=0.0001\n",
      "[18] time=751.12, avg_loss=0.0001, train_err=0.0004\n",
      "Eval: test_mse=0.0001\n",
      "[19] time=750.52, avg_loss=0.0001, train_err=0.0004\n",
      "Eval: test_mse=0.0001\n",
      "[20] time=750.10, avg_loss=0.0001, train_err=0.0004\n",
      "Eval: test_mse=0.0001\n",
      "[21] time=751.48, avg_loss=0.0001, train_err=0.0004\n",
      "Eval: test_mse=0.0001\n",
      "[22] time=754.78, avg_loss=0.0001, train_err=0.0004\n",
      "Eval: test_mse=0.0001\n",
      "[23] time=750.91, avg_loss=0.0001, train_err=0.0004\n",
      "Eval: test_mse=0.0001\n",
      "[24] time=752.77, avg_loss=0.0001, train_err=0.0004\n",
      "Eval: test_mse=0.0001\n",
      "[25] time=753.65, avg_loss=0.0001, train_err=0.0004\n",
      "Eval: test_mse=0.0001\n",
      "[26] time=750.76, avg_loss=0.0001, train_err=0.0004\n",
      "Eval: test_mse=0.0001\n",
      "[27] time=749.92, avg_loss=0.0001, train_err=0.0004\n",
      "Eval: test_mse=0.0001\n",
      "[28] time=748.84, avg_loss=0.0001, train_err=0.0004\n",
      "Eval: test_mse=0.0001\n",
      "[29] time=750.18, avg_loss=0.0001, train_err=0.0004\n",
      "Eval: test_mse=0.0001\n",
      "[30] time=751.62, avg_loss=0.0001, train_err=0.0004\n",
      "Eval: test_mse=0.0001\n",
      "[31] time=751.99, avg_loss=0.0001, train_err=0.0003\n",
      "Eval: test_mse=0.0001\n",
      "[32] time=733.47, avg_loss=0.0001, train_err=0.0003\n",
      "Eval: test_mse=0.0001\n",
      "[33] time=731.80, avg_loss=0.0001, train_err=0.0003\n",
      "Eval: test_mse=0.0001\n",
      "[34] time=732.10, avg_loss=0.0001, train_err=0.0003\n",
      "Eval: test_mse=0.0001\n",
      "[35] time=735.33, avg_loss=0.0001, train_err=0.0003\n",
      "Eval: test_mse=0.0001\n",
      "[36] time=729.84, avg_loss=0.0001, train_err=0.0003\n",
      "Eval: test_mse=0.0001\n",
      "[37] time=731.30, avg_loss=0.0001, train_err=0.0003\n",
      "Eval: test_mse=0.0001\n",
      "[38] time=731.39, avg_loss=0.0001, train_err=0.0003\n",
      "Eval: test_mse=0.0001\n",
      "[39] time=736.52, avg_loss=0.0001, train_err=0.0003\n",
      "Eval: test_mse=0.0001\n",
      "[40] time=733.18, avg_loss=0.0001, train_err=0.0003\n",
      "Eval: test_mse=0.0001\n",
      "[41] time=732.13, avg_loss=0.0001, train_err=0.0003\n",
      "Eval: test_mse=0.0001\n",
      "[42] time=695.13, avg_loss=0.0001, train_err=0.0003\n",
      "Eval: test_mse=0.0001\n",
      "[43] time=644.47, avg_loss=0.0001, train_err=0.0003\n",
      "Eval: test_mse=0.0001\n",
      "[44] time=636.06, avg_loss=0.0001, train_err=0.0003\n",
      "Eval: test_mse=0.0001\n",
      "[45] time=609.39, avg_loss=0.0001, train_err=0.0003\n",
      "Eval: test_mse=0.0001\n",
      "[46] time=606.42, avg_loss=0.0001, train_err=0.0003\n",
      "Eval: test_mse=0.0001\n",
      "[47] time=606.64, avg_loss=0.0001, train_err=0.0003\n",
      "Eval: test_mse=0.0001\n",
      "[48] time=606.08, avg_loss=0.0001, train_err=0.0003\n",
      "Eval: test_mse=0.0001\n",
      "[49] time=607.34, avg_loss=0.0001, train_err=0.0003\n",
      "Eval: test_mse=0.0001\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train_err': 0.00032004316663369534,\n",
       " 'avg_loss': 8.001079165842383e-05,\n",
       " 'avg_lasso_loss': None,\n",
       " 'epoch_train_time': 607.3449381000028,\n",
       " 'test_mse': tensor(9.9853e-05)}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6. Train\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Optimizer, scheduler, loss\n",
    "optimizer = optim.Adam(operator.parameters(), lr=1e-3)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "def mse_loss_dict(out, **kwargs):\n",
    "    y = kwargs[\"y\"]\n",
    "    return F.mse_loss(out, y, reduction=\"mean\")\n",
    "\n",
    "trainer = Trainer(model=operator, n_epochs=n_epochs, verbose=True)\n",
    "\n",
    "trainer.train(\n",
    "    train_loader=train_loader,\n",
    "    test_loaders={\"test\": test_loader},\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    regularizer=False,\n",
    "    training_loss=mse_loss_dict,\n",
    "    eval_losses={\"mse\": mse_loss_dict}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfa8858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Save model\n",
    "operator.save_checkpoint(save_folder=\"./checkpoints\", save_name=\"fno3d_reactiondiff_bc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b888078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation on last 150 samples:\n",
      "MSE : 0.000208\n",
      "MAE : 0.008181\n",
      "R²  : 0.995152\n"
     ]
    }
   ],
   "source": [
    "# 8. Test on unseen data\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "# Select last 150 samples as test set\n",
    "test_dataset = dataset\n",
    "test_indices = np.arange(len(test_dataset) - 150, len(test_dataset))\n",
    "test_subset = Subset(test_dataset, test_indices)\n",
    "test_loader = DataLoader(test_subset, batch_size=1, shuffle=False)\n",
    "\n",
    "# Parameters\n",
    "device = \"cpu\"\n",
    "operator.to(device)\n",
    "operator.eval()\n",
    "\n",
    "mse_list = []\n",
    "mae_list = []\n",
    "r2_list = []\n",
    "\n",
    "def r2_score(y_true, y_pred):\n",
    "    ss_res = torch.sum((y_true - y_pred) ** 2)\n",
    "    ss_tot = torch.sum((y_true - torch.mean(y_true)) ** 2)\n",
    "    return 1 - ss_res / ss_tot\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x, y_true in test_loader:\n",
    "\n",
    "        x = x.to(device)              # Shape: [1, C, Nx, Ny, 10]\n",
    "        y_true = y_true.to(device)    # Shape: [1, C, Nx, Ny, 10]\n",
    "\n",
    "        y_pred = operator(x)                   # → same shape as y_true\n",
    "\n",
    "        # Compute metrics on tensor (no flatten needed)\n",
    "        mse_val = torch.mean((y_true - y_pred) ** 2).item()\n",
    "        mae_val = torch.mean(torch.abs(y_true - y_pred)).item()\n",
    "        r2_val = r2_score(y_true, y_pred).item()\n",
    "\n",
    "        mse_list.append(mse_val)\n",
    "        mae_list.append(mae_val)\n",
    "        r2_list.append(r2_val)\n",
    "\n",
    "print(\"Evaluation on last 150 unseen samples:\")\n",
    "print(f\"MSE : {np.mean(mse_list):.6f}\")\n",
    "print(f\"MAE : {np.mean(mae_list):.6f}\")\n",
    "print(f\"R²  : {np.mean(r2_list):.6f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
